{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data_path = Path.cwd().parent / \"data\" / \"raw\" / \"foqa.csv\"\n",
    "assert annotated_data_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_dict = load_dataset(\"alexandrainst/foqa\", \"all-samples\")\n",
    "raw_df = pd.concat(\n",
    "    [raw_dataset_dict[split].to_pandas() for split in raw_dataset_dict.keys()]\n",
    ")\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_lengths = raw_df.original_context.str.len().sort_values()\n",
    "context_lengths.hist(bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_shorter_context(answer_dict: dict, context: str) -> str:\n",
    "    \"\"\"Extracts a shorter context containing the answer.\n",
    "\n",
    "    This ensures that the number of characters in the context is\n",
    "    at most 5000 characters.\n",
    "\n",
    "    Args:\n",
    "        answer_dict:\n",
    "            The answer dictionary, with keys 'text' and 'answer_start'.\n",
    "        context:\n",
    "            The context.\n",
    "\n",
    "    Returns:\n",
    "        The shortened context.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError:\n",
    "            If it wasn't possible to shorten the context.\n",
    "    \"\"\"\n",
    "    # If the context is short enough already then we don't have to shorten it\n",
    "    if len(context) < 5000:\n",
    "        return context\n",
    "\n",
    "    answer = answer_dict[\"text\"][0]\n",
    "\n",
    "    paragraphs = [p for p in context.split(\"\\n\\n\") if p]\n",
    "    paragraph_answer_idx = next(\n",
    "        idx for idx, paragraph in enumerate(paragraphs) if answer in paragraph\n",
    "    )\n",
    "    single_paragraph_is_short_enough = len(paragraphs[paragraph_answer_idx]) < 5000\n",
    "    assert answer in paragraphs[paragraph_answer_idx]\n",
    "\n",
    "    # If the paragraph containing the answer is short enough, then we identify all the\n",
    "    # possible longest contexts with the answer, by including paragraphs above and below that\n",
    "    # paragraph. We then select the context at random from these.\n",
    "    if single_paragraph_is_short_enough:\n",
    "        all_valid_contexts = set()\n",
    "        for start_paragraph_idx in range(paragraph_answer_idx, -1, -1):\n",
    "            best_candidate_context = paragraphs[paragraph_answer_idx]\n",
    "            for end_paragraph_idx in range(paragraph_answer_idx, len(paragraphs)):\n",
    "                candidate_context = \"\\n\\n\".join(\n",
    "                    paragraphs[start_paragraph_idx : end_paragraph_idx + 1]\n",
    "                )\n",
    "                if candidate_context and len(candidate_context) < 5000:\n",
    "                    best_candidate_context = candidate_context\n",
    "                else:\n",
    "                    break\n",
    "            all_valid_contexts.add(best_candidate_context)\n",
    "        assert len(all_valid_contexts) > 0\n",
    "        context = random.choice(list(all_valid_contexts))\n",
    "        return context\n",
    "\n",
    "    # Otherwise, we start splitting up the paragraph containing the answer into lines\n",
    "    lines = paragraphs[paragraph_answer_idx].split(\"\\n\")\n",
    "    line_answer_idx = next(idx for idx, line in enumerate(lines) if answer in line)\n",
    "    assert answer in lines[line_answer_idx]\n",
    "\n",
    "    # Again, we do the same as we did for the paragraphs, just for lines instead\n",
    "    all_valid_contexts = set()\n",
    "    for start_line_idx in range(line_answer_idx, -1, -1):\n",
    "        best_candidate_context = \"\"\n",
    "        for end_line_idx in range(line_answer_idx, len(lines)):\n",
    "            candidate_context = \"\\n\".join(lines[start_line_idx : end_line_idx + 1])\n",
    "            if candidate_context and len(candidate_context) < 5000:\n",
    "                best_candidate_context = candidate_context\n",
    "            else:\n",
    "                break\n",
    "        all_valid_contexts.add(best_candidate_context)\n",
    "    assert len(all_valid_contexts) > 0\n",
    "    return random.choice(list(all_valid_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_answer_start(context: str, answer_dict: dict) -> dict:\n",
    "    \"\"\"Updates the start of the answers.\n",
    "\n",
    "    Args:\n",
    "        context:\n",
    "            The context where the answer appears.\n",
    "        answer_dict:\n",
    "            The original answer dictionary, with keys 'text' and 'answer_start'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with keys 'text' and 'answer_start', each being\n",
    "        lists with a single element.\n",
    "    \"\"\"\n",
    "    answer = answer_dict[\"text\"][0]\n",
    "    answer_start = context.index(answer)\n",
    "    assert context[answer_start : answer_start + len(answer)] == answer\n",
    "    return dict(text=[answer], answer_start=[answer_start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"original_context\" not in raw_df.columns:\n",
    "    raw_df.rename(columns=dict(context=\"original_context\"), inplace=True)\n",
    "raw_df[\"context\"] = raw_df.apply(\n",
    "    lambda x: extract_shorter_context(\n",
    "        answer_dict=x.answers, context=x.original_context\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "raw_df[\"answers\"] = raw_df.apply(\n",
    "    lambda x: update_answer_start(context=x.context, answer_dict=x.answers), axis=1\n",
    ")\n",
    "shorter_context_lengths = raw_df.context.str.len().sort_values()\n",
    "shorter_context_lengths.hist(bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = pd.read_csv(annotated_data_path)\n",
    "annotated_df.columns = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"context\",\n",
    "    \"question\",\n",
    "    \"deleteme\",\n",
    "    \"validation\",\n",
    "    \"answers\",\n",
    "]\n",
    "annotated_df.drop(columns=[\"deleteme\", \"answers\"], inplace=True)\n",
    "annotated_df[\"answers\"] = raw_df.answers\n",
    "annotated_df[\"context\"] = raw_df.context\n",
    "annotated_df = annotated_df[raw_df.drop(columns=\"original_context\").columns]\n",
    "annotated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_texts = [\"correct\", \"corrected\"]\n",
    "correct_df = annotated_df.query(\"validation in @correct_texts\")\n",
    "correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = correct_df.query('validation == \"corrected\"')\n",
    "num_test_samples_missing = 1024 - len(test_df)\n",
    "test_df = pd.concat(\n",
    "    [\n",
    "        test_df,\n",
    "        correct_df.loc[\n",
    "            [idx for idx in correct_df.index if idx not in test_df.index]\n",
    "        ].sample(n=num_test_samples_missing),\n",
    "    ]\n",
    ")\n",
    "val_df = correct_df.loc[\n",
    "    [idx for idx in correct_df.index if idx not in test_df.index]\n",
    "].sample(n=128)\n",
    "train_df = correct_df.loc[\n",
    "    [\n",
    "        idx\n",
    "        for idx in correct_df.index\n",
    "        if idx not in test_df.index and idx not in val_df.index\n",
    "    ]\n",
    "]\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "test = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "DatasetDict(dict(train=train, val=val, test=test)).push_to_hub(\n",
    "    \"alexandrainst/foqa\", \"default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_texts = [\"incorrect\", \"incorrect-answer\"]\n",
    "incorrect_df = annotated_df.query(\"validation in @incorrect_texts\").dropna()\n",
    "\n",
    "wrong_corrected_df = raw_df.drop(columns=\"original_context\").loc[\n",
    "    annotated_df.query('validation == \"corrected\"').index.tolist()\n",
    "]\n",
    "wrong_corrected_df.validation = annotated_df.loc[\n",
    "    wrong_corrected_df.index.tolist()\n",
    "].validation\n",
    "\n",
    "incorrect_df = pd.concat(objs=[incorrect_df, wrong_corrected_df])\n",
    "incorrect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_pandas(incorrect_df, preserve_index=False).push_to_hub(\n",
    "    \"alexandrainst/foqa\", \"incorrect-samples\"\n",
    ")\n",
    "Dataset.from_pandas(raw_df, preserve_index=False).push_to_hub(\n",
    "    \"alexandrainst/foqa\", \"all-samples\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
